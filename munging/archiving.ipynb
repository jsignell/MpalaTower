{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archiving workspace for dealing with messy backup files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ipython notebook contains several functions for exploring backup files, investigating their start and end dates, cleaning them up, and outputting them to a more succinct .dat files. Run all of the function definitions before trying to process anything new. I am working on adding funcitonality that would set the cap length of a file to something approachable and workable like 200MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified by: Julia Signell\n",
    "\n",
    "Date created: 2014-11-10\n",
    "\n",
    "Date modified: 2015-04-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import sys\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILEDIR = 'E:/TowerDataArchive/'\n",
    "input_dir = FILEDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR1000_SN5709_Table1_2012_05_21.dat\n",
      "CR1000_SN5709_Table1_2010_04_23.dat\n",
      "CR1000_SN5709_Table1_2010_07_18.dat\n",
      "CR3000_SN3557_Table1_2012_10_27.dat\n",
      "CR3000_SN3557_Table1_2010_02_21.dat\n",
      "CR3000_SN3557_Table1_2010_04_19.dat\n",
      "CR3000_SN3557_Table1_2011_07_07.dat\n",
      "CR3000_SN3557_Table1_2012_10_25.dat\n",
      "CR3000_SN3557_Table1_2010_07_19.dat\n",
      "CR3000_SN3557_Table1_2010_04_13.dat\n",
      "CR3000_SN3557_Table1_2010_02_08.dat\n",
      "CR3000_SN3557_Table1_2011_12_31.dat\n",
      "CR3000_SN3557_Table1_2012_02_28.dat\n",
      "CR3000_SN3557_Table1_2010_07_20.dat\n",
      "CR3000_SN3557_Table1_2010_03_09.dat\n",
      "CR3000_SN5355_Table1_2014_09_02.dat\n",
      "CR3000_SN5355_Table1_2012_10_29.dat\n"
     ]
    }
   ],
   "source": [
    "def search_and_replace(f,search,replace):\n",
    "    #checks a file (f) for the search term and replaces it inplace \n",
    "    import fileinput\n",
    "\n",
    "    for line in fileinput.input(f, inplace=True):\n",
    "        print(line.replace(search,replace),end = '')\n",
    "        \n",
    "for f in os.listdir(input_dir):\n",
    "    if 'Table1' in f:\n",
    "        print(f)\n",
    "        search_and_replace(input_dir+f,'Gass','Grass')\n",
    "        search_and_replace(input_dir+f,'open','Open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR1000_SN5709_Table1_2012_05_21.dat\n",
      "CR1000_SN5709_Table1_2010_04_23.dat\n",
      "CR1000_SN5709_Table1_2010_07_18.dat\n",
      "CR3000_SN3557_Table1_2012_10_27.dat\n",
      "CR3000_SN3557_Table1_2010_02_21.dat\n",
      "CR3000_SN3557_Table1_2010_04_19.dat\n",
      "CR3000_SN3557_Table1_2011_07_07.dat\n",
      "CR3000_SN3557_Table1_2012_10_25.dat\n",
      "CR3000_SN3557_Table1_2010_07_19.dat\n",
      "CR3000_SN3557_Table1_2010_04_13.dat\n",
      "CR3000_SN3557_Table1_2010_02_08.dat\n",
      "CR3000_SN3557_Table1_2011_12_31.dat\n",
      "CR3000_SN3557_Table1_2012_02_28.dat\n",
      "CR3000_SN3557_Table1_2010_07_20.dat\n",
      "CR3000_SN3557_Table1_2010_03_09.dat\n",
      "CR3000_SN5355_Table1_2014_09_02.dat\n",
      "CR3000_SN5355_Table1_2012_10_29.dat\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(input_dir):\n",
    "    if 'Table1' in f:\n",
    "        print(f)\n",
    "        inFile = open(input_dir+f, 'rU')\n",
    "        lines = inFile.readlines()\n",
    "        if 'Gass' in lines[1]: print('still there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createDF(fileDir, df_dataName, data):\n",
    "    #make a dataframe of file names with some file meta data in the columns\n",
    "    files =[]\n",
    "    for fileName in os.listdir(fileDir+data+'/'):\n",
    "        if fileName not in open(fileDir+'processed2netCDF.txt','r').read():\n",
    "        #if data in fileName:\n",
    "            files.append(fileName)\n",
    "            df_dataName=pd.DataFrame(index=[files], columns=['start','end','length','version'])\n",
    "    return df_dataName, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dateRE = re.compile(r'(\\d{4})-(\\d{2})-(\\d{2})')\n",
    "dateStringRE = re.compile(r'(\\d{4})-(\\d{2})-(\\d{2}) (\\d{2}):(\\d{2}):(\\d{2})\\.?(\\d)?')\n",
    "def parseCampbellDate(dateString):\n",
    "    date = dateStringRE.match(dateString)\n",
    "    try:\n",
    "        return ([int(date.group(j)) for j in range(1,7)]+[int(date.group(7))*100000])\n",
    "    except:\n",
    "        return ([int(date.group(j)) for j in range(1,7)])\n",
    "\n",
    "def numericDate(dateString):\n",
    "    return datetime.datetime(*parseCampbellDate(dateString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open a file and figure out when it starts and ends\n",
    "def daterange(fileDir,fileName,df_dataName): \n",
    "    print fileName\n",
    "    inFile = open(fileDir+fileName,'rU')\n",
    "    lines = inFile.readlines()\n",
    "    try:\n",
    "        start = lines[4][1:20]\n",
    "        start = numericDate(start)\n",
    "    except: start = nan      \n",
    "    try: \n",
    "        end = lines[-1][1:20]\n",
    "        end = numericDate(end)\n",
    "    except: end = nan\n",
    "    df_dataName['start'][fileName]=start\n",
    "    df_dataName['end'][fileName]=end\n",
    "    try: length = end-start\n",
    "    except: length = nan\n",
    "    df_dataName['length'][fileName]=length\n",
    "    df_dataName['version'][fileName]=lines[0].partition('CPU:')[2].partition('\"')[0]\n",
    "    return df_dataName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process(fileDir,data,df_dataName):\n",
    "    df_dataName, files = createDF(fileDir, df_dataName, data)\n",
    "    for fileName in files:\n",
    "        try:df_dataName = daterange(fileDir, fileName,df_dataName)\n",
    "        except: continue\n",
    "    df_dataName = df_dataName.sort(columns = ['version','start','length'], ascending = [True,True,False])\n",
    "    df_dataName.to_csv(fileDir+ '%s/'%data + '%s_summary.csv' %data)\n",
    "    \n",
    "    #df_dataName[['start']] = df_dataName[['start']].drop_duplicates()\n",
    "    df_dataName = df_dataName.dropna(how ='any')\n",
    "    goagain =True\n",
    "    while goagain ==True:\n",
    "        goagain = False\n",
    "        i=0\n",
    "        while i in(range(len(df_dataName.index)-1)):\n",
    "            j = df_dataName.iloc[i,:]\n",
    "            k = df_dataName.iloc[i+1,:]\n",
    "            if k['version'] == j['version'] and k['start']<=j['end'] and k['end']<=j['end']:\n",
    "                df_dataName = df_dataName.drop(k.name)\n",
    "                goagain = True\n",
    "            i+=1\n",
    "        print goagain\n",
    "    print 'done!'\n",
    "    return df_dataName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mergeFiles(fileDir,data,df_dataName):\n",
    "    fileNames = df.index\n",
    "    day = str(df_dataName['start'][fileNames[0]]).replace('-','_').partition(' ')[0]\n",
    "    number = fileNames[0].split(data)[0]\n",
    "    outFile = open(fileDir.partition('towerraw/')[0]+number+data+'_'+day+'.dat','w')\n",
    "    for i in range(len(fileNames)-1):\n",
    "        oldFile = open(fileDir+data+'/'+fileNames[i],'rU')\n",
    "        oldlines = oldFile.readlines()\n",
    "        newFile = open(fileDir+data+'/'+fileNames[i+1],'rU')\n",
    "        newlines = newFile.readlines()\n",
    "        if oldlines[0:4] != newlines[0:4]:\n",
    "            outFile.close()\n",
    "            day = str(df_dataName['start'][fileNames[i+1]]).replace('-','_').partition(' ')[0]\n",
    "            number = fileNames[i+1].split(data)[0]\n",
    "            outFile = open(fileDir.partition('towerraw/')[0]+number+data+'_'+day+'.dat','w')\n",
    "            for line in newlines:\n",
    "                print >>outFile, line,\n",
    "        elif oldlines[0:4] == newlines[0:4]:       \n",
    "            for line in newlines[4:]:\n",
    "                if line not in oldlines:\n",
    "                    print >>outFile, line,\n",
    "    print 'done merging!'\n",
    "    outFile.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''def mergeFiles(fileDir,data,df_dataName):\n",
    "    versions = list(df_dataName['version'].unique())\n",
    "    k = 0\n",
    "    number=list()\n",
    "    for version in versions:\n",
    "        outFile = open(fileDir+data+'_'+version+'.dat','w')\n",
    "        i = 0\n",
    "        for fileName in df_dataName.index:\n",
    "            if df_dataName['version'][fileName] == version:\n",
    "                print 'merging' + fileName\n",
    "                inFile = open(fileDir+data+'/'+fileName,'rU')\n",
    "                lines = inFile.readlines()\n",
    "                if i ==0:\n",
    "                    number.append(fileName.split(data)[0])\n",
    "                    print >>outFile, lines[0],lines[1],lines[2],lines[3],\n",
    "                    i+=1\n",
    "                if not(lines[0][0:6] == '\"TOA5\"'): raise Exception ('file %s does not have a header'%fileName)\n",
    "                for i in range(4,len(lines)):\n",
    "                    print >>outFile, lines[i],\n",
    "        outFile.close()\n",
    "        lines_seen = set() # holds lines already seen\n",
    "        outfile = open(fileDir.partition('towerraw/')[0]+number[k]+data+'_'+version+'.dat', \"w\")\n",
    "        k+=1\n",
    "        for line in open(fileDir+data+'_'+version+'.dat', \"r\"):\n",
    "            if line not in lines_seen: # not a duplicate\n",
    "                outfile.write(line)\n",
    "                lines_seen.add(line)\n",
    "        os.remove(fileDir+data+'_'+version+'.dat')\n",
    "        outfile.close()\n",
    "    print 'done!''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is where you need to start paying attention. If you have already run everything above this, then you should be ready to actually process files. This is where you need to input the name of the data value that you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR1000_SN13341_licor6262.dat\n",
      "CR1000_SN13341_licor6262.dat.1.backup\n",
      "CR1000_SN13341_licor6262.dat.2.backup\n",
      "CR1000_SN13341_licor6262.dat.3.backup\n",
      "CR1000_SN13341_licor6262.dat.backup\n",
      "CR1000_SN13341_licor6262_2012_10_29.dat\n",
      "CR1000_SN13341_licor6262_AVG_2013_01_08.dat\n",
      "CR800_SN4735_licor6262.dat\n",
      "False"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-acd2a52a4463>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileDir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmergeFiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileDir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-148-0b8e71bf3e4e>\u001b[0m in \u001b[0;36mmergeFiles\u001b[1;34m(fileDir, data, df_dataName)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileNames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0moldFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileDir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfileNames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0moldlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moldFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mnewFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileDir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfileNames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mnewlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "fileDir = 'C:/Users/Julia/Dropbox (PE)/KenyaLab/Data/Tower/TowerDataArchive/towerraw/'\n",
    "df = pd.DataFrame\n",
    "data = 'licor'\n",
    "for fileName in os.listdir(fileDir):\n",
    "    if data in fileName:\n",
    "        try:\n",
    "            shutil.copy(fileDir+fileName,fileDir+ '%s/'%data+fileName)\n",
    "            os.remove(fileDir+fileName)\n",
    "        except: continue\n",
    "df = process(fileDir,data,df)\n",
    "mergeFiles(fileDir,data,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CR1000_SN7276_flux_2010_02_04.dat\n",
      "CR1000_SN7276_flux_2010_02_05.dat\n",
      "2010-02-09 11:00:00 missing until 2010-02-09 12:00:00\n",
      "2010-02-19 10:00:00 missing until 2010-02-19 11:00:00\n",
      "2010-02-19 11:00:00 missing until 2010-02-19 12:30:00\n",
      "2010-02-20 09:30:00 missing until 2010-02-20 10:30:00\n",
      "2010-03-06 01:00:00 missing until 2010-03-06 09:30:00\n",
      "CR1000_SN7276_flux_2010_04_01.dat\n",
      "2010-04-09 12:30:00 missing until 2010-04-09 13:30:00\n",
      "2010-04-11 04:30:00 missing until 2010-04-11 05:30:00\n",
      "2010-04-11 12:00:00 missing until 2010-04-11 13:00:00\n",
      "2010-04-11 14:00:00 missing until 2010-04-11 15:00:00\n",
      "2010-04-11 19:30:00 missing until 2010-04-11 20:30:00\n",
      "2010-04-12 04:30:00 missing until 2010-04-12 05:30:00\n",
      "2010-04-12 19:00:00 missing until 2010-04-12 20:00:00\n",
      "2010-04-12 22:30:00 missing until 2010-04-12 23:30:00\n",
      "2010-04-13 05:00:00 missing until 2010-04-13 06:00:00\n",
      "2010-04-13 10:00:00 missing until 2010-04-13 11:00:00\n",
      "CR1000_SN7276_flux_2010_04_26.dat\n",
      "CR1000_SN7276_flux_2010_04_27.dat\n",
      "CR1000_SN7276_flux_2010_05_05.dat\n",
      "2010-05-06 09:30:00 missing until 2010-05-06 12:00:00\n",
      "CR1000_SN7276_flux_2010_06_03.dat\n",
      "CR1000_SN7276_flux_2010_06_07.dat\n",
      "2010-06-21 17:30:00 missing until 2010-06-21 18:30:00\n",
      "CR3000_SN4709_flux_2010_07_18.dat\n",
      "2010-07-26 16:00:00 missing until 2010-07-27 10:30:00\n",
      "2010-08-05 08:30:00 missing until 2010-08-05 16:00:00\n",
      "2010-11-22 09:30:00 missing until 2010-11-25 11:00:00\n",
      "2010-12-16 15:30:00 missing until 2011-01-03 16:30:00\n",
      "CR3000_SN4709_flux_2011_04_24.dat\n",
      "2011-05-06 05:00:00 missing until 2011-05-06 15:00:00\n",
      "CR3000_SN4709_flux_2011_05_06.dat\n",
      "CR3000_SN4709_flux_2011_05_08.dat\n",
      "2011-05-10 05:30:00 missing until 2011-05-10 10:30:00\n",
      "CR3000_SN4709_flux_2011_12_02.dat\n",
      "CR3000_SN4709_flux_2011_12_07.dat\n",
      "2011-12-08 05:30:00 missing until 2011-12-08 07:00:00\n",
      "2012-02-21 15:00:00 missing until 2012-02-21 16:00:00\n",
      "CR3000_SN4709_flux_2012_02_28.dat\n",
      "2012-04-19 11:00:00 missing until 2012-04-19 13:00:00\n",
      "cr3000_sn4709_flux_2012_06_15.dat\n",
      "CR3000_SN4709_flux_2012_10_26.dat\n",
      "TOA5_4709.flux_2010_04_23.dat\n"
     ]
    }
   ],
   "source": [
    "fileDir = \"C:/Users/Julia/Dropbox (PE)/KenyaLab/Data/Tower/TowerDataArchive/\"\n",
    "data = 'flux'\n",
    "for fileName in os.listdir(fileDir):\n",
    "    if data in fileName:\n",
    "        print fileName\n",
    "        df1 = pd.read_csv(fileDir+fileName,header=[1,2,3], parse_dates = 0,index_col = 0)\n",
    "        for i in range(0,len(df1.index)-1):\n",
    "            if df1.index[i+1]-df1.index[i] > datetime.timedelta(0, 1800):\n",
    "                print df1.index[i],'missing until',df1.index[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>RECORD</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DOM</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Second</th>\n",
       "      <th>uSecond</th>\n",
       "      <th>WeekDay</th>\n",
       "      <th>Day_of_Year</th>\n",
       "      <th>...</th>\n",
       "      <th>Solar_Wm2_Avg</th>\n",
       "      <th>Solar_MJ_Tot</th>\n",
       "      <th>Solar_Wm2_1_Avg</th>\n",
       "      <th>Solar_MJ_1_Tot</th>\n",
       "      <th>e_hmp_Avg</th>\n",
       "      <th>e_hmp_Std</th>\n",
       "      <th>t_hmp_Avg</th>\n",
       "      <th>t_hmp_Std</th>\n",
       "      <th>rh_hmp_Avg</th>\n",
       "      <th>rh_hmp_Std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS</th>\n",
       "      <th>RN</th>\n",
       "      <th>Unnamed: 2_level_1</th>\n",
       "      <th>Unnamed: 3_level_1</th>\n",
       "      <th>Unnamed: 4_level_1</th>\n",
       "      <th>Unnamed: 5_level_1</th>\n",
       "      <th>Unnamed: 6_level_1</th>\n",
       "      <th>Unnamed: 7_level_1</th>\n",
       "      <th>Unnamed: 8_level_1</th>\n",
       "      <th>Unnamed: 9_level_1</th>\n",
       "      <th>Unnamed: 10_level_1</th>\n",
       "      <th>...</th>\n",
       "      <th>W/m2</th>\n",
       "      <th>MJ/m2</th>\n",
       "      <th>W/m2</th>\n",
       "      <th>MJ/m2</th>\n",
       "      <th>kPa</th>\n",
       "      <th>kPa</th>\n",
       "      <th>C</th>\n",
       "      <th>C</th>\n",
       "      <th>percent</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Unnamed: 1_level_2</th>\n",
       "      <th>Smp</th>\n",
       "      <th>Smp</th>\n",
       "      <th>Smp</th>\n",
       "      <th>Smp</th>\n",
       "      <th>Smp</th>\n",
       "      <th>Smp</th>\n",
       "      <th>Smp</th>\n",
       "      <th>Smp</th>\n",
       "      <th>Smp</th>\n",
       "      <th>...</th>\n",
       "      <th>Avg</th>\n",
       "      <th>Tot</th>\n",
       "      <th>Avg</th>\n",
       "      <th>Tot</th>\n",
       "      <th>Avg</th>\n",
       "      <th>Std</th>\n",
       "      <th>Avg</th>\n",
       "      <th>Std</th>\n",
       "      <th>Avg</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-07-07 14:00:00</th>\n",
       "      <td> 0</td>\n",
       "      <td> 2011</td>\n",
       "      <td> 7</td>\n",
       "      <td> 7</td>\n",
       "      <td> 14</td>\n",
       "      <td>  0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 5</td>\n",
       "      <td> 188</td>\n",
       "      <td>...</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-07 14:10:00</th>\n",
       "      <td> 1</td>\n",
       "      <td> 2011</td>\n",
       "      <td> 7</td>\n",
       "      <td> 7</td>\n",
       "      <td> 14</td>\n",
       "      <td> 10</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 5</td>\n",
       "      <td> 188</td>\n",
       "      <td>...</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-07 14:20:00</th>\n",
       "      <td> 2</td>\n",
       "      <td> 2011</td>\n",
       "      <td> 7</td>\n",
       "      <td> 7</td>\n",
       "      <td> 14</td>\n",
       "      <td> 20</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 5</td>\n",
       "      <td> 188</td>\n",
       "      <td>...</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-07 14:30:00</th>\n",
       "      <td> 3</td>\n",
       "      <td> 2011</td>\n",
       "      <td> 7</td>\n",
       "      <td> 7</td>\n",
       "      <td> 14</td>\n",
       "      <td> 30</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 5</td>\n",
       "      <td> 188</td>\n",
       "      <td>...</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-07 14:40:00</th>\n",
       "      <td> 4</td>\n",
       "      <td> 2011</td>\n",
       "      <td> 7</td>\n",
       "      <td> 7</td>\n",
       "      <td> 14</td>\n",
       "      <td> 40</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 5</td>\n",
       "      <td> 188</td>\n",
       "      <td>...</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> NAN</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "TIMESTAMP                        RECORD                Year  \\\n",
       "TS                                   RN  Unnamed: 2_level_1   \n",
       "                     Unnamed: 1_level_2                 Smp   \n",
       "2011-07-07 14:00:00                   0                2011   \n",
       "2011-07-07 14:10:00                   1                2011   \n",
       "2011-07-07 14:20:00                   2                2011   \n",
       "2011-07-07 14:30:00                   3                2011   \n",
       "2011-07-07 14:40:00                   4                2011   \n",
       "\n",
       "TIMESTAMP                         Month                 DOM  \\\n",
       "TS                   Unnamed: 3_level_1  Unnamed: 4_level_1   \n",
       "                                    Smp                 Smp   \n",
       "2011-07-07 14:00:00                   7                   7   \n",
       "2011-07-07 14:10:00                   7                   7   \n",
       "2011-07-07 14:20:00                   7                   7   \n",
       "2011-07-07 14:30:00                   7                   7   \n",
       "2011-07-07 14:40:00                   7                   7   \n",
       "\n",
       "TIMESTAMP                          Hour              Minute  \\\n",
       "TS                   Unnamed: 5_level_1  Unnamed: 6_level_1   \n",
       "                                    Smp                 Smp   \n",
       "2011-07-07 14:00:00                  14                   0   \n",
       "2011-07-07 14:10:00                  14                  10   \n",
       "2011-07-07 14:20:00                  14                  20   \n",
       "2011-07-07 14:30:00                  14                  30   \n",
       "2011-07-07 14:40:00                  14                  40   \n",
       "\n",
       "TIMESTAMP                        Second             uSecond  \\\n",
       "TS                   Unnamed: 7_level_1  Unnamed: 8_level_1   \n",
       "                                    Smp                 Smp   \n",
       "2011-07-07 14:00:00                   0                   0   \n",
       "2011-07-07 14:10:00                   0                   0   \n",
       "2011-07-07 14:20:00                   0                   0   \n",
       "2011-07-07 14:30:00                   0                   0   \n",
       "2011-07-07 14:40:00                   0                   0   \n",
       "\n",
       "TIMESTAMP                       WeekDay          Day_of_Year  \\\n",
       "TS                   Unnamed: 9_level_1  Unnamed: 10_level_1   \n",
       "                                    Smp                  Smp   \n",
       "2011-07-07 14:00:00                   5                  188   \n",
       "2011-07-07 14:10:00                   5                  188   \n",
       "2011-07-07 14:20:00                   5                  188   \n",
       "2011-07-07 14:30:00                   5                  188   \n",
       "2011-07-07 14:40:00                   5                  188   \n",
       "\n",
       "TIMESTAMP                   ...           Solar_Wm2_Avg  Solar_MJ_Tot  \\\n",
       "TS                          ...                    W/m2         MJ/m2   \n",
       "                            ...                     Avg           Tot   \n",
       "2011-07-07 14:00:00         ...                     NAN           NAN   \n",
       "2011-07-07 14:10:00         ...                     NAN           NAN   \n",
       "2011-07-07 14:20:00         ...                     NAN           NAN   \n",
       "2011-07-07 14:30:00         ...                     NAN           NAN   \n",
       "2011-07-07 14:40:00         ...                     NAN           NAN   \n",
       "\n",
       "TIMESTAMP            Solar_Wm2_1_Avg  Solar_MJ_1_Tot  e_hmp_Avg  e_hmp_Std  \\\n",
       "TS                              W/m2           MJ/m2        kPa        kPa   \n",
       "                                 Avg             Tot        Avg        Std   \n",
       "2011-07-07 14:00:00              NAN             NAN          0          0   \n",
       "2011-07-07 14:10:00              NAN             NAN          0          0   \n",
       "2011-07-07 14:20:00              NAN             NAN          0          0   \n",
       "2011-07-07 14:30:00              NAN             NAN          0          0   \n",
       "2011-07-07 14:40:00              NAN             NAN          0          0   \n",
       "\n",
       "TIMESTAMP            t_hmp_Avg  t_hmp_Std rh_hmp_Avg rh_hmp_Std  \n",
       "TS                           C          C    percent    percent  \n",
       "                           Avg        Std        Avg        Std  \n",
       "2011-07-07 14:00:00          0          0          0          0  \n",
       "2011-07-07 14:10:00          0          0          0          0  \n",
       "2011-07-07 14:20:00          0          0          0          0  \n",
       "2011-07-07 14:30:00          0          0          0          0  \n",
       "2011-07-07 14:40:00          0          0          0          0  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileDir = 'C:/Users/Julia/Dropbox (PE)/KenyaLab/Data/Tower/TowerDataArchive/towerraw/'\n",
    "df = pd.DataFrame\n",
    "data = 'upper'\n",
    "for fileName in os.listdir(fileDir):\n",
    "    if data in fileName:\n",
    "        try:\n",
    "            shutil.copy(fileDir+fileName,fileDir+ '%s/'%data+fileName)\n",
    "            os.remove(fileDir+fileName)\n",
    "        except: continue\n",
    "df = process(fileDir,data,df)\n",
    "mergeFiles(fileDir,data,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "for root, dirs, files in os.walk('E:/'):\n",
    "    for i in files:\n",
    "        if 'upper' in i:\n",
    "            print root+'/'+i\n",
    "            k+=1\n",
    "print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archiving.ipynb\n",
      "CR1000_SN13341_licor_2012_09_16.dat\n",
      "CR3000_SN4709_flux_2010_04_23.dat\n",
      "CR3000_SN4709_WVIA_2012_10_23.dat\n",
      "CR3000_SN4709_WVIA_2014_05_27.dat\n",
      "desktop.ini\n",
      "processed2netCDF.txt\n",
      "test"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 13] Permission denied: 'C:/Users/Julia/Dropbox (PE)/KenyaLab/Data/Tower/TowerDataArchive/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-8fd3de52045b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_dataName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileDir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_dataName\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfileName\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdf_dataName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdaterange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileDir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileName\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_dataName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-a0c8f65ad5ac>\u001b[0m in \u001b[0;36mdaterange\u001b[1;34m(fileDir, fileName, df_dataName)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdaterange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileDir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfileName\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_dataName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mfileName\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0minFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileDir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfileName\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 13] Permission denied: 'C:/Users/Julia/Dropbox (PE)/KenyaLab/Data/Tower/TowerDataArchive/test'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATALOC = \"E:/TowerDataArchive/\"\n",
    "fileDir = DATALOC\n",
    "df_dataName = pd.DataFrame\n",
    "df_dataName, files = createDF(fileDir, df_dataName,'')\n",
    "for fileName in files:\n",
    "    df_dataName = daterange(fileDir, fileName,df_dataName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
