{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Meta Data\n",
    "*************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains everything you need to create a nice neat list of meta data dictionaries out of netcdf files. In this case we have made one meta data dictionary for each day in a five year span. The dictionaries are only created when there is data available on the given day, and there are up to 8 datafiles represented on each day. Each files contains data from various sensors and that is reported out in a whole slew of variables. Each variable has attributes associated with it in the netcdf file. These attributes are carried over into the dict and other attributes are added, such as a flag variable that can be raised for various problematic data situations (missing data, unreasonable data, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Overview of Data Dict Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is the output to be a dict for each of the following: Variable, File, Metadata. The functions that generate each of these will be called from the parse_netcdf function. Then the dicts will be fed into the classes and output to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import os\n",
    "import xray\n",
    "from posixpath import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flask.ext.mongoengine import MongoEngine\n",
    "\n",
    "db = MongoEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOTDIR = 'C:/Users/Julia/Documents/GitHub/MpalaTower/raw_netcdf_output/'\n",
    "data = 'Table1'\n",
    "\n",
    "datas = ['upper', 'Table1', 'lws', 'licor6262', 'WVIA',\n",
    "         'Manifold', 'flux', 'ts_data', 'Table1Rain']\n",
    "non_static_attrs = ['instrument', 'source', 'program', 'logger']\n",
    "static_attrs = ['station_name', 'lat', 'lon', 'elevation',\n",
    "                'Year', 'Month', 'DOM', 'Minute', 'Hour',\n",
    "                'Day_of_Year', 'Second', 'uSecond', 'WeekDay']\n",
    "\n",
    "# Setting expected ranges for units. It is ok to include multiple ways of writing\n",
    "# the same unit, just put all the units in a list\n",
    "flag_by_units = {}\n",
    "\n",
    "temp_min = 0\n",
    "temp_max = 40\n",
    "temp = ['Deg C', 'C']\n",
    "for unit in temp:\n",
    "    flag_by_units.update({unit : {'min' : temp_min, 'max' : temp_max}})\n",
    "    \n",
    "percent_min = 0\n",
    "percent_max = 100\n",
    "percent = ['percent', '%']\n",
    "for unit in percent:\n",
    "    flag_by_units.update({unit : {'min' : percent_min, 'max' : percent_max}})\n",
    "\n",
    "shf_min = ''\n",
    "shf_max = ''\n",
    "shf = ['W/m^2']\n",
    "\n",
    "shf_cal_min = ''\n",
    "shf_cal_max = ''\n",
    "shf_cal = ['W/(m^2 mV)']\n",
    "\n",
    "batt_min = 11\n",
    "batt_max = 240\n",
    "batt = ['Volts', 'V']\n",
    "for unit in batt:\n",
    "    flag_by_units.update({unit : {'min' : batt_min, 'max' : batt_max}})\n",
    "\n",
    "PA_min = 15\n",
    "PA_max = 25\n",
    "PA = ['uSec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_netcdf(input_dir, data, f, static_attrs):\n",
    "    ds = xray.Dataset()\n",
    "    ds = xray.open_dataset(join(input_dir, data, f),\n",
    "                           decode_cf=True, decode_times=True)\n",
    "    df = ds.to_dataframe()\n",
    "\n",
    "    # drop from df, columns that don't change with time\n",
    "    exclude = [var for var in static_attrs if var in df.columns]\n",
    "    df_var = df.drop(exclude, axis=1)  # dropping vars like lat, lon\n",
    "\n",
    "    # get some descriptive statistics on each of the variables\n",
    "    df_summ = df_var.describe()\n",
    "    return ds, df_summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Variable(db.EmbeddedDocument):\n",
    "    timestep_count = db.IntField()\n",
    "    flags = db.ListField(db.StringField())\n",
    "    name = db.StringField(db_field='var')\n",
    "    units = db.StringField()\n",
    "    count = db.IntField()\n",
    "    avg_val = db.FloatField(db_field='mean')  # Avoid function names\n",
    "    std_val = db.FloatField(db_field='std')\n",
    "    min_val = db.FloatField(db_field='min')\n",
    "    max_val = db.FloatField(db_field='max')\n",
    "    p25th = db.FloatField(db_field='25%')\n",
    "    p75th = db.FloatField(db_field='75%')\n",
    "    content_type = db.StringField(db_field='content_coverage_type')\n",
    "    coordinates = db.StringField()\n",
    "    comment = db.StringField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_flags(var, flag_by_units):\n",
    "    # check status of data and raise flags\n",
    "    flags = []\n",
    "\n",
    "    if var.timestep_count*11/12 < var.count < var.timestep_count:\n",
    "        flags.append('missing a little data')\n",
    "    elif var.timestep_count < var.count <= var.timestep_count*11/12:\n",
    "        flags.append('missing some data')\n",
    "    elif var.timestep_count/12 <= var.count <= var.timestep_count/2:\n",
    "        flags.append('missing lots of data')\n",
    "    elif var.count == 0:\n",
    "        flags.append('no data')\n",
    "\n",
    "    try:\n",
    "        if var.name.startswith('del'):\n",
    "            pass\n",
    "        elif var.comment == 'Std':  # don't check std_dev\n",
    "            pass\n",
    "        else:\n",
    "            if var.max_val > flag_by_units[var.units]['max']:\n",
    "                flags.append('contains high values')\n",
    "            if var.min_val < flag_by_units[var.units]['min']:\n",
    "                flags.append('contains low values')\n",
    "    except:\n",
    "        pass\n",
    "    return flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_variable(ds, df_summ, var, flag_by_units):\n",
    "    if df_summ[var]['count'] != 0:\n",
    "        this_variable = Variable(\n",
    "            name=var,\n",
    "            timestep_count=len(ds['time']),\n",
    "            count=df_summ[var]['count'],\n",
    "            avg_val=df_summ[var]['mean'],\n",
    "            std_val=df_summ[var]['std'],\n",
    "            min_val=df_summ[var]['min'],\n",
    "            p25th=df_summ[var]['25%'],\n",
    "            p75th=df_summ[var]['75%'],\n",
    "            max_val=df_summ[var]['max'],\n",
    "            units=ds[var].attrs['units'],\n",
    "            comment=ds[var].attrs['comment'],\n",
    "            coordinates=ds[var].attrs['coordinates'],\n",
    "            content_type=ds[var].attrs['content_coverage_type'],\n",
    "        )\n",
    "    else:\n",
    "        this_variable = Variable(\n",
    "            name=var,\n",
    "            timestep_count=len(ds['time']),\n",
    "            count=df_summ[var]['count']\n",
    "        )\n",
    "    this_variable.flags = generate_flags(this_variable, flag_by_units)\n",
    "\n",
    "    return this_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Datafile attribute dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class File(db.EmbeddedDocument):\n",
    "\n",
    "    source = db.StringField()\n",
    "    instrument = db.StringField()\n",
    "    datafile = db.StringField()\n",
    "    filename = db.StringField()\n",
    "    frequency = db.FloatField()\n",
    "    frequency_flag = db.StringField()\n",
    "\n",
    "    # The File object contains a list of Variables:\n",
    "    variables = db.EmbeddedDocumentListField(Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_to_sec(num, units):\n",
    "    if units.startswith(('Min','min')):\n",
    "        out = int(num)*60\n",
    "    elif units.startswith(('ms', 'mS')):\n",
    "        out = float(num)/1000\n",
    "    elif units.startswith(('s','S')):\n",
    "        out = int(num)\n",
    "    else:\n",
    "        print('couldn\\'t parse units')\n",
    "        return (num, units)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def programmed_frequency(f, this_file):\n",
    "    data = f['data']\n",
    "    program = this_file.source.split('CPU:')[1].split(',')[0]\n",
    "    try:\n",
    "        prog = open(join(f['dir'], 'programs', program))\n",
    "    except:\n",
    "        freq_flag = 'program: %s not found' % program\n",
    "        freq = float('nan')\n",
    "        return freq_flag, freq\n",
    "    lines = prog.readlines()\n",
    "    i= 0\n",
    "    k = 0\n",
    "    interval = None\n",
    "    DT = 'DataTable'\n",
    "    DI = 'DataInterval'\n",
    "    CT = 'CallTable'\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].split()[0:] == [DT, data]:\n",
    "            k = i\n",
    "        if lines[i].split()[0:1] == DI and i <= (k+2):\n",
    "            interval = lines[i].split(',')[1]\n",
    "            units = lines[i].split(',')[2]\n",
    "        i +=1\n",
    "    if interval == None:\n",
    "        i = 0\n",
    "        for i in range(len(lines)):\n",
    "            if lines[i].split()[0:1] == 'Scan':\n",
    "                interval = lines[i].split('(')[1].split(',')[0]\n",
    "                units = lines[i].split(',')[1]\n",
    "            if lines[i].split()[0:2] == [CT, data] and i <= (k+7):\n",
    "                interval = interval\n",
    "                units = units\n",
    "            else:\n",
    "                interval = None\n",
    "                units = None\n",
    "            i +=1\n",
    "    if interval == None:\n",
    "        freq_flag = 'could not find interval in %s' % program\n",
    "        freq = 'nan'\n",
    "        return freq_flag, freq\n",
    "    try:\n",
    "        num = int(interval)\n",
    "    except:\n",
    "        for line in lines:\n",
    "            if line.startswith('Const '+interval):\n",
    "                a = line.split('=')[1]\n",
    "                b = a.split()[0]\n",
    "                num = int(b)\n",
    "    freq = convert_to_sec(num, units)\n",
    "    freq_flag = 'found frequency'\n",
    "    \n",
    "    return freq_flag, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_file(f, ds, df_summ, flag_by_units):\n",
    "    this_file = File(\n",
    "            source=ds.attrs['source'],\n",
    "            instrument=ds.attrs['instrument'],\n",
    "            filename=f['filename']\n",
    "        )\n",
    "    freq_flag, freq = programmed_frequency(f, this_file)\n",
    "    this_file.frequency = float(freq)\n",
    "    this_file.frequency_flag = freq_flag\n",
    "    variables = []\n",
    "    for var in df_summ:\n",
    "        variables.append(generate_variable(ds, df_summ, var, flag_by_units))\n",
    "    this_file.variables = variables\n",
    "    \n",
    "    return this_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Metadata(db.Document):\n",
    "\n",
    "    license = db.StringField()\n",
    "    title = db.StringField()\n",
    "    creator = db.StringField(db_field='creator_name', default='Kelly Caylor')\n",
    "    creator_email = db.EmailField()\n",
    "    institution = db.StringField()\n",
    "    aknowledgements = db.StringField()\n",
    "    feature_type = db.StringField(db_field='featureType')\n",
    "    year = db.IntField(required=True)\n",
    "    month = db.IntField(required=True)\n",
    "    doy = db.IntField(required=True)\n",
    "    date = db.DateTimeField(required=True)\n",
    "    summary = db.StringField()\n",
    "    conventions = db.StringField()\n",
    "    naming_authority = db.StringField()  # or URLField?\n",
    "\n",
    "    # The Metadata object contains a list of Files:\n",
    "    files = db.EmbeddedDocumentListField(File)\n",
    "\n",
    "    meta = {\n",
    "        'collection': 'metadata',\n",
    "        'ordering': ['-date'],\n",
    "        'index_background': True,\n",
    "        'indexes': [\n",
    "            'year',\n",
    "            'month',\n",
    "            'doy',\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_metadata(self, input_dir, ds):\n",
    "    \n",
    "    self.license = ds.attrs['license']\n",
    "    self.title = ds.attrs['title']\n",
    "    self.creator=ds.attrs['creator_name']\n",
    "    self.creator_email=ds.attrs['creator_email']\n",
    "    self.institution=ds.attrs['institution']\n",
    "    self.aknowledgements=ds.attrs['acknowledgement']\n",
    "    self.feature_type=ds.attrs['featureType']\n",
    "    self.summary=ds.attrs['summary']\n",
    "    self.conventions=ds.attrs['Conventions']\n",
    "    self.naming_authority=ds.attrs['naming_authority']\n",
    "    \n",
    "    return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Process data into list of daily data dicts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_dict = {'year': 2010,                 \n",
    "             'doy': 001,\n",
    "             'month': 01\n",
    "             'date': 2010-01-01,          \n",
    "             'files': [{'ROOTDIR': ROOTDIR, 'data': data, 'f': f},\n",
    "                       {'ROOTDIR': ROOTDIR, 'data': data, 'f': f},\n",
    "                       {'ROOTDIR': ROOTDIR, 'data': data, 'f': f}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_dates(self, input_dir, datas):\n",
    "    data_list = []\n",
    "    data_dict = None\n",
    "    start = '2010-01-01'\n",
    "    end = dt.datetime.utcnow()\n",
    "    rng = pd.date_range(start, end, freq='D')\n",
    "    for date in rng:\n",
    "        i = 0\n",
    "        y = date.year\n",
    "        m = date.month\n",
    "        d = date.dayofyear\n",
    "        f = 'raw_MpalaTower_%i_%03d.nc' % (y, d)\n",
    "        if any(f in os.listdir(join(input_dir, data)) for data in datas):\n",
    "            data_dict = {'year': y, 'month' : m, 'doy': d, 'date' : date, 'files': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_files(this_metadata, datas):\n",
    "    f = 'raw_MpalaTower_%i_%03d.nc' % (this_metadata.year, this_metadata.doy)\n",
    "    for data in datas:\n",
    "        if f in os.listdir(join(input_dir, data)):\n",
    "            this_file.datafile = data\n",
    "            this_file.filename = f\n",
    "            this_metadata.files.append.(this_file)\n",
    "    return this_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Send to internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MongoClient('dogen.mongohq.com', 10097)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flask.ext.mongoengine import MongoEngine\n",
    "\n",
    "db = MongoEngine()\n",
    "db.connect(host='mongodb://joey:joejoe@dogen.mongohq.com:10097/mpala_tower_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_MpalaTower_2015_097.nc upper\n",
      "raw_MpalaTower_2015_098.nc upper\n",
      "raw_MpalaTower_2015_098.nc Table1\n",
      "raw_MpalaTower_2015_098.nc flux\n",
      "raw_MpalaTower_2015_099.nc upper\n",
      "raw_MpalaTower_2015_099.nc Table1\n",
      "raw_MpalaTower_2015_099.nc lws\n",
      "raw_MpalaTower_2015_099.nc Manifold\n",
      "raw_MpalaTower_2015_099.nc flux\n",
      "raw_MpalaTower_2015_100.nc upper\n",
      "raw_MpalaTower_2015_100.nc Table1\n",
      "raw_MpalaTower_2015_100.nc lws\n",
      "raw_MpalaTower_2015_100.nc WVIA\n",
      "raw_MpalaTower_2015_100.nc Manifold\n",
      "raw_MpalaTower_2015_100.nc flux\n"
     ]
    },
    {
     "ename": "AutoReconnect",
     "evalue": "[Errno 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAutoReconnect\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-244-c19f146926cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mthis_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_summ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag_by_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mthis_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mthis_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\mongoengine\\document.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, force_insert, validate, clean, write_concern, cascade, cascade_kwargs, _refs, save_condition, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                     \u001b[0mobject_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mwrite_concern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m                     \u001b[0mobject_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mwrite_concern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[0mobject_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pymongo\\collection.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, to_save, manipulate, safe, check_keys, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"_id\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mto_save\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmanipulate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m             self.update({\"_id\": to_save[\"_id\"]}, to_save, True,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pymongo\\collection.pyc\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, doc_or_docs, manipulate, safe, check_keys, continue_on_error, **kwargs)\u001b[0m\n\u001b[0;32m    413\u001b[0m             message._do_batched_insert(self.__full_name, gen(), check_keys,\n\u001b[0;32m    414\u001b[0m                                        \u001b[0msafe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontinue_on_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m                                        self.uuid_subtype, client)\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_one\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pymongo\\mongo_client.pyc\u001b[0m in \u001b[0;36m_send_message\u001b[1;34m(self, message, with_last_error, command)\u001b[0m\n\u001b[0;32m   1136\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mConnectionFailure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1138\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAutoReconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1139\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m                 \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAutoReconnect\u001b[0m: [Errno 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond"
     ]
    }
   ],
   "source": [
    "    ds, df_summ = process_netcdf(data_dict['files'][0]['dir'],\n",
    "                                 data_dict['files'][0]['data'],\n",
    "                                 data_dict['files'][0]['filename'],\n",
    "                                 static_attrs)\n",
    "    this_metadata = generate_metadata(data_dict, ds)\n",
    "    for f in data_dict['files']:\n",
    "        print(f['filename'],f['data'])\n",
    "        ds, df_summ = process_netcdf(f['dir'], f['data'], f['filename'], static_attrs)\n",
    "        this_file = generate_file(f, ds, df_summ, flag_by_units)\n",
    "        this_metadata.files.append(this_file)\n",
    "    this_metadata.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
